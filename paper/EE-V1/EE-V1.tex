\documentclass[10pt,journal,compsoc]{IEEEtran}

\ifCLASSOPTIONcompsoc
  % IEEE Computer Society needs nocompress option
  % requires cite.sty v4.0 or later (November 2003)
  \usepackage[nocompress]{cite}
\else
  % normal IEEE
  \usepackage{cite}
\fi

\usepackage{url}
\usepackage{apacite}
\usepackage{booktabs}
\usepackage{array}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{svg}

\begin{document}

\title{TCformer: Temporal Convolution Transformer Are Efficient For Time Series Prediction}

\author{Brian~Zu}

% make the title area
\maketitle

\IEEEraisesectionheading{\section{Introduction}\label{sec:introduction}}

\IEEEPARstart{T}{ime} series prediction is a critical task in various domains such as finance, healthcare, weather forecasting, and transportation. It uses historical data to predict future. In recent years, attention mechanism was well known after being implemented in the transformer architecture \cite{attention-is-all-you-need}. Since 2017, there have been many works around this model in many tasks such as natural language processing \cite{few-shot-learners}, and computer vision \cite{vit} given its performance.  In the field of time series prediction, many researchers found that the standard Transformer architecture may not fully exploit the temporal relationships and dependencies present in time series data. So there many variants of transformers appeared, including informer \cite{informer}, patchTST \cite{patchtst}, that all aim to improve its performance. 


\section{Related work}

\subsection{Transformer architecture}

Despite the impressing works above, the transformer architecture comes with its great size. Many other works in training large scaled transformer models appeared with up to 600B weights that cost for 2048 TPU v3 cores \cite{Lepikhin2020-jx}. Recent work \cite{pmlr-v119-li20m} has reduced the size with compression with the cost of precision. In fact, currently researchers have no way to get around with the bulky model without compromises. Seeing this flaw, we proposed a new hybrid architecture TCformer that both uses CNN and Transformer to achieve an improvement in precision with reduction in size for multivariate time series prediction task. 

Considering the architecture of Transformer, different from convolution layers, its parameter size is inherently dependent on the size of the input. In Transformer, the scaled dot-product attention \cite{attention-is-all-you-need} computes the output as:

\begin{gather}
    Attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt{d_k}}V)
\end{gather}

Where the $Q$ represents the query matrix, $K$ the key matrix, and $V$ value matrix. In time series prediction task, those matrices are 4D tensors that has a shape of:

\begin{gather}
        Q,K,V\in \mathbb{R}^{[d_b \times d_s \times d_f]}
\end{gather}

where $d_b$ is the batch size, $d_s$ the sequence length, $d_f$ the numbers of features. Thus, they are all dependent on the input size. Even though it can be addressed through a linear layer that maps the dimension lower, it's not a common approach due to the huge information loss through this process which decreases the model's precision. This attention mechanism itself already made this model large, not to mention the parameters in the FFN (Feed Forward Network). 

With that being said, reducing the dimension while remaining the precision of attention mechanism became the key in reducing the model's size. 

\subsection{Convolution-based architectures}

Though Transformer has gained the most focus after its success in Large Language Models (LLMs) and was mostly researched on in doing various tasks including times series, convolution has also been applied on time series \cite{hewage2020temporal}. From the two base mechanisms, the time series models have diverged as more models are proposed based on them. Moreover, recent work \cite{cnntransformercompare} has shown that CNN are more accurate than Transformer model, reaching $92\%$ accuracy compared to the $80\%$ accuracy of Transformer. 

However, pure convolutional architectures are hard to deal with time series data. For each kernel, relatively small to the whole sequence, can only see a very limited range of data and can have a overview of a long series only when the network is deep enough and the pieces of traits converges at the top of the network. Thus its hard for those architectures expand it's capacity in processing long sequences. 

\subsection{Convolution attention combined architectures}

The combination of convolution layers with attention mechanism isn't a rare approach. In ConTNet, researchers have embedded the encoding block in between the convolution layers \cite{yan2021contnetuseconvolutiontransformer} for computer vision task. In time series prediction, Informer \cite{informer} has changed the encoder by adding convolution layers between the attention layers for handling longer time series sequences. 

Unlike previous works, the TCformer we proposed neither embed convolutional layer in between the model nor apply it on the whole input. 

\section{TCformer}

The TCformer predicts the next timestamp $X_{n+1}$ by observing the historical data $X_1,..., X_n$, and it's based on the assumption that the closer the nearer the historical data is to $X_{n+1}$ the more relevant it is. This applies to most of the time series data, such as stocks, traffic, electricity and so on. 

\subsection{Structure Overview}

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{images/StructureDiagram.png}
    \caption{TCformer structure}
    \label{fig:TCformer-structure}
\end{figure}

The TCformer has a simple and efficient structure. On fig \ref{fig:TCformer-structure}, the split factor is a scalar factor that splits the dataset into to on the dimension of sequence length. For example, a scalar factor $\lambda=0.8$ would split the input of shape $X\in \mathbb{R}^{b,f,s}$ into $X\in \mathbb{R}^{b,f,0.8\cdot s}$ and $X\in \mathbb{R}^{b,f,0.2\cdot s}$ where:

\begin{enumerate}
    \item $b$ is the batch size
    \item $f$ is the dimension of feature
    \item $s$ is the sequence length
\end{enumerate}

The Conv1d layer is a 1-Dimensional convolution layer that extract traits from the $80\%$ of the input and reduces the dimension. It can be replaced by a Conv2d. The right branch leaves the raw data that will be joined back with the left branch in the addition component. In addition component the two branches' tensors will be merged on the third dimension $s$. Lastly, the joined tensor will be the input for the Transformer encoder block. 

\subsection{Convolution summery}

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{images/Left branch.png}
    \caption{Convolutional sequence summary on left branch}
    \label{fig:convolutional-operation}
\end{figure}

In the left branch, Conv1d layer is outputting summaries for the first $80\%$ of earlier data, leaving the rest $20\%$ raw for the Transformer encoder to process. This will cause the input $X$ to change it's third dimension's shape $s$ from:

\begin{gather}
    s_1=0.8\cdot s\\
    s_2=\lfloor\frac{0.8\cdot s-k}{r} \rfloor+1
\end{gather}

\begin{enumerate}
    \item $s_1$ is the sequence length before convolution layer
    \item $k$ is kernel size
    \item $s_2$ is the sequence length after convolution layer
    \item $r$ is the stride
\end{enumerate}

On the right branch, the raw data is not processed. Given that the Transformer has already have a good performance on raw data, applying convolution is not needed and might reduce the information perceived in the encoder block. 

\subsection{Parameter reduction}

After the addition, the input for the encoder block will have a sequence length of:

\begin{gather}
    s_3=s_2+0.2\cdot s\\
    =\lfloor\frac{0.8\cdot s-k}{r} \rfloor+1+0.2\cdot s
\end{gather}

The reduction of sequence length $s_3$ compared to $s$ can be calculated:

\begin{gather}
    l=s-s_3\\
    =s-\lfloor\frac{0.8\cdot s-k}{r} \rfloor-1-0.2\cdot s
\end{gather}

Notice $l$ is positively related with $k$ and $r$, changing the stride and kernel size can give a flexible control on the model size. In the attention block, a reduction of $l$ will reduce:

\begin{gather}
    l\times b\times f
\end{gather}

in each of the $Q,K,V$ matrices, largely reducing it's parameters.

\section{Experiments}

We had conducted experiments on 3 real world datasets in the experiment, including weather used in Autoformer \cite{autoformer} and AMD stock data. 

\subsection{Dataset}

The weather dataset included 52696 rows of data with 21 covariates. 

The AMD stock dataset is accessed through Baostock API \cite{baostock}. It includes 10995 rows of data with 5 covariates. The data spans from 2nd of January 1981 to 14th of August 2024. 

\subsection{Experiment setup}

Transformer model has a hidden dimension of 8, 2 layers, 2 heads for attention. For all models the optimizer uses Adam, with lr=1e-3, wd=1e-4, dropout=0.1, training in 5 epochs with batch size of 512 with train test $9:1$ splitting. The TCformer uses split factor of 0.8 with the CNN using kernel size 6 with a stride of 6. The criteria uses MSE (Mean Squared Error). 

\subsection{Results}

\begin{table}[h]
\centering
\begin{tabular}{rrrr}
\toprule
TCformer1d & transformer & seq\_len & pct\_improve(\%) \\
\midrule
0.018490 & 0.073744 & 16 & 25.07 \\
0.015125 & 0.061805 & 32 & 24.47 \\
0.012830 & 0.053022 & 64 & 24.20 \\
0.017049 & 0.069303 & 128 & 24.60 \\
0.016919 & 0.070958 & 256 & 23.84 \\
0.017332 & 0.072824 & 512 & 23.80 \\
\bottomrule \\
\end{tabular}
\caption{AMD stock data test MSE loss}
\label{tab:comparison}
\end{table}

In the AMD stock data, the TCformer with 1d in average has improved performance by about 25\%, while having reduced size of model. During experiment, the TCformer was successfully trained on dataset when sequence length is 1024 however the Transformer model failed due to GPU memory overflow.  

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{images/AMD-loss-cmp.png}
    \caption{AMD loss curve}
    \label{fig:AMD-loss-curve}
\end{figure}

\begin{table}[h]
\centering
\begin{tabular}{rrrr}
\toprule
TCformer1d & transformer & seq\_len & pct\_improve (\%) \\
\midrule
0.026144 & 0.104435 & 16 & 25.03 \\
0.026144 & 0.104253 & 32 & 25.08 \\
0.026188 & 0.103928 & 64 & 25.20 \\
0.025542 & 0.101652 & 128 & 25.13 \\
0.028019 & 0.107837 & 256 & 25.98 \\
0.026371 & 0.104923 & 512 & 25.13 \\
\bottomrule\\
\end{tabular}
\caption{Weather data test MSE loss}
\label{tab:comparison}
\end{table}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{images/weather-loss-cmp.png}
    \caption{Weather loss curve}
    \label{fig:weather-loss-curve}
\end{figure}

% \appendices
% \section{Appendix title}
% Appendix one text goes here.

% \section{Appendix title}
% Appendix two text goes here.

\section{Conclusion}
The TCformer architecture overall is successful in addressing the limitations of both pure convolutional and transformer models. By integrating temporal convolution with transformer-based attention, TCformer achieves superior predictive performance while maintaining computational efficiency. The model's ability to handle longer sequences and its consistent performance improvements across different datasets suggest its potential for wide applicability in various time series prediction tasks. Future work could explore the model's performance on a broader range of datasets and investigate further optimizations to enhance its efficiency and scalability.


\ifCLASSOPTIONcaptionsoff
  \newpage
\fi

\bibliography{references}
\bibliographystyle{apacite}

\end{document}


